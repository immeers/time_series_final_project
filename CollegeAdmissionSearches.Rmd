---
title: "College Admission Searches"
author: "Imogen Meers, Sarah Deussing, & Sarah Cernugel"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
subtitle: "Will the Top-Ranked Schools Change?"
---

## Objective
What is the objective of performing time series forecasting and who would it benefit? 
This analysis will forecast the number of Google Searches for admissions data of the top five nationally ranked U.S. universities. The current U.S. News rankings are as follows:

  1. Princeton University
  2. Massachusetts Institute of Technology
  3. Harvard University
  4. Stanford University
  5. Yale University

Data was obtained from Google Search statistics for each school; the search terms are "[school name] admission." Each dataset contains two columns: the date and the number of searches for the term. These are weekly time series with five years of data.

Time-series analysis will be performed to identify trends and seasonality in each dataset. Following, a forecast will predict future weekly search amounts. Doing so will determine if specific schools are projected to rise or fall in popularity. 

It is likely that a school projected to see a spike in admissions search numbers in the next year, may have an increase in the number of applications, and thus the difficulty of admission. This would be useful information to determine changes in admission rate and exclusivity of schools in the future.

The audience for this project are high school juniors and seniors, and admission counselors going through the college application process. These students can use the forecasted search numbers and possible ranking changes to determine which schools they should apply to and which may be more or less competitive in the next year. This project can be expanded to include any set of schools in which a person wants to apply. 

## Initial Data Analysis

The first step in the time series process is to check if each dataset is a random walk. If so, this means that the best possible forecast for this dataset is a naive forecast and there is no need to apply any modelling. The following code does so for the top 5 schools' data.
```{r, message=FALSE, warning =FALSE}
library(forecast)
library(readr)
library(ggplot2)
library(stringr)
library(zoo)
library(Metrics)

file_paths = Sys.glob(file.path("./data", "*"))
data_sets = list()
college_names = list()

for (i in (1:length(file_paths))){
  
  college.data <- read_delim(file_paths[i], delim = ",", skip = 1)
  
  college_names[i] = str_extract(string= file_paths[i], pattern = "[a-z]*_admission")
  
  colnames(college.data) <- c("Week", "Searches")
  
  college.ts <- ts(college.data$Searches, start = c(2020, 5), end = c(2025, 5), freq = 52) #weekly with a yearly season
  
  g = autoplot(college.ts) +
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  data_sets[[i]] <- college.ts
}
```


```{r Check for random walk}
for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  diff.ts <- diff(college.ts)
  
  print(Acf(diff.ts, main = college_names[i]))
}
```
All ACF plots show significant correlation after differencing meaning these time series are not random walks.

To further prove this, we will also perform hypothesis testing.

```{r}
for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  college.ar.1 <- Arima(college.ts, order = c(1, 0, 0))
  
  print(college_names[i])
  print(summary(college.ar.1))
  
  ar1_intercept <- summary(college.ar.1)$coef["ar1"]
  ar1_se <- sqrt(diag(college.ar.1$var.coef))['ar1']
  
  print(paste("Is significant?" , -1.96 > (ar1_intercept-1)/ar1_se))
}

```

We can conclude that all our coefficients are significantly different from 0. 

Null Hypothesis: beta = 1 (i.e., random walk)

Alternative Hypothesis: beta not equal to 1 (i.e., not random walk)

To be specific, our t-stat = (coefficient - 1)/s.e = 
t-stat = +-/1.96 at 95% confidence interval

All t-stat are < -1.96 so are significant, there is significant evidence to reject null hypothesis. These time series are not random walks.

## Time Series Process

For each school, we will perform the following process.

  1. Determine the best time time series for the data. Validation data will be the last 52 weeks (one year).
  
    - Seasonal Naive
    - Simple Exponential Smoothing
    - TSLM : Time Series Linear Model
    - Holt-Winters
    - Auto ARIMA
    - NNAR
  
  2. Find the best model for each school (comparing MAPE)
  3. Using the entire dataset as training and the best model from the previous step, forecast statistics for the next year (52 weeks).

The forecasts for each school will help determine how each school may rise or fall in popularity. 

## Time Series Process

For each school, we will perform the following process.

  1. Determine the best time time series for the data. Validation data will be the last 52 weeks (one year).
  
    - Seasonal Naive
    - Simple Exponential Smoothing
    - TSLM : Time Series Linear Model
    - Holt-Winters
    - Auto ARIMA
    - Moving Average
    - Holts Winters
    - Auto Arima
    - NNAR
  
  2. Find the best model for each school (comparing MAPE)
  3. Using the entire dataset as training and the best model from the previous step, forecast statistics for the next year (52 weeks).

The forecasts for each school will help determine how each school may rise or fall in popularity. 

#### Train/Valid Split
```{r, message = FALSE}
nValid <- 52
plots <- list()

train_data <- list()
valid_data <- list()
n_valid <- list()

for (i in 1:length(file_paths)) {
  
  college.ts <- data_sets[[i]]
  
  nTrain <- length(college.ts) - nValid
  
  train.ts <- window(college.ts, start = c(2020, 5), end = c(2020, nTrain))
  valid.ts <- window(college.ts, start = c(2020, nTrain + 1), end = c(2020, nTrain + nValid))
  
  # create train/valid for each school
  school <- sub("_.*", "", college_names[i])
  
  train_data[[school]] <- train.ts
  valid_data[[school]] <- valid.ts
  n_valid[[school]] <- nValid
  
  p <- autoplot(train.ts) + 
    autolayer(valid.ts) +
    ggtitle(paste("Train Test Split: ", school))+
    ylab("Interest")
  
  plots[[i]] <- p
}

plots
```

#### Seasonal Naive - SC
```{r}
# Initialize lists to store models and MAPE values
models <- list()
mape_values <- list()

for (i in 1:length(file_paths)) {
  
  # Existing code to split data into train/valid
  college.ts <- data_sets[[i]]
  nTrain <- length(college.ts) - nValid
  train.ts <- window(college.ts, start = c(2020, 5), end = c(2020, nTrain))
  valid.ts <- window(college.ts, start = c(2020, nTrain + 1), end = c(2020, nTrain + nValid))
  
  school <- sub("_.*", "", college_names[i])
  train_data[[school]] <- train.ts
  valid_data[[school]] <- valid.ts
  n_valid[[school]] <- nValid
  
  # Fit a model (e.g., ARIMA, Exponential Smoothing, etc.)
  model <- auto.arima(train.ts)  # Using auto.arima as an example
  
  # Save the model in the models list with the school name
  models[[school]] <- model
  
  # Predict on validation set
  forecast_values <- forecast(model, h = nValid)
  
  # Calculate MAPE (Mean Absolute Percentage Error)
  mape_value <- mape(valid.ts, forecast_values$mean)
  
  # Save MAPE value in the mape_values list with the school name
  mape_values[[school]] <- mape_value
  
  # Plot train vs valid data (unchanged from your original code)
  p <- autoplot(train.ts) + 
    autolayer(valid.ts) +
    ggtitle(paste("Train Test Split: ", school)) +
    ylab("Interest")
  
  plots[[i]] <- p
}

# Output the models and MAPE values
models
mape_values
```
```{r}
seasonal.naive.forecast <- snaive(train.ts, h = nValid)

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]

  seasonal.naive <- snaive(train.ts, h = nValid)

  # create predictions
  snaive.pred <- forecast(seasonal.naive, h = n_valid[[i]])
  
  mape_value <- mape(valid.ts, snaive.pred$mean)
  mape_values[[school]] <- mape_value

  g = autoplot(college.ts) +
  autolayer(snaive.pred$mean, series = "Seasonal Naive")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}

mape_values
```

#### Simple Exponential Smoothing - SC
```{r}
#Typically used for data that has no trend or seasonality
ses <- ets(train.ts, model = "ANN", alpha = 0.5)

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]

  ses <- ets(train.ts, model = "ANN", alpha = 0.5)

  # create predictions
  ses.pred <- forecast(ses, h = n_valid[[i]])
  
  mape_value <- mape(valid.ts, ses.pred$mean)
  mape_values[[school]] <- mape_value

  g = autoplot(college.ts) +
  autolayer(snaive.pred$mean, series = "Simple Exponential Smoothing")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}

mape_values
```

#### TSLM : Time Series Linear Model - SC
```{r}
#Linear Trend
train.lm <- tslm(train.ts ~ trend)
#Exponential Trend
train.lm.expo.trend <- tslm(train.ts ~ trend, lambda = 0)
#Quadratic Trend
train.lm.poly.trend <- tslm(train.ts ~ trend + I(trend^2))
#Additive Seasonality
train.lm.season <- tslm(train.ts ~ season)
#Additive Seasonality and Linear Trend
train.lm.trend.season <- tslm(train.ts ~ trend + season)
#Additive Seasonality and Quadratic Trend
train.lm.trend.season <- tslm(train.ts ~ trend + I(trend^2) + season)
```

#### Moving Average - IM
```{r}
for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  ma.trailing <- rollmean(college.ts, k = 12, align = "right") #zoo
  ma.centered <- ma(college.ts, order = 12) #forecast
  
  g = autoplot(college.ts) +
  autolayer(ma.trailing, series="Trailing MA")+
  autolayer(ma.centered, series="Centered MA")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}
```

#### Holts Winters - IM
```{r}
for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]

  hwin.mod <- ets(train.ts, model = "MAA")

  # create predictions
  hwin.pred <- forecast(hwin.mod, h = n_valid[[i]], level = 0)

  g = autoplot(college.ts) +
  autolayer(hwin.pred$mean, series = "Holt-Winters")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}
```

#### Auto Arima - SD

#### NNAR - SD
