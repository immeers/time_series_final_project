---
title: "College Admission Searches"
author: "Imogen Meers, Sarah Deussing, & Sarah Cernugel"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
subtitle: "Will the Top-Ranked Schools Change?"
---

## Objective
What is the objective of performing time series forecasting and who would it benefit? 
This analysis will forecast the number of Google Searches for admissions data of the top five nationally ranked U.S. universities. The current U.S. News rankings are as follows:

  1. Princeton University
  2. Massachusetts Institute of Technology
  3. Harvard University
  4. Stanford University
  5. Yale University

Data was obtained from Google Search statistics for each school; the search terms are "[school name] admission." Each dataset contains two columns: the date and the number of searches for the term. These are weekly time series with five years of data.

Time-series analysis will be performed to identify trends and seasonality in each dataset. Following, a forecast will predict future weekly search amounts. Doing so will determine if specific schools are projected to rise or fall in popularity. 

It is likely that a school projected to see a spike in admissions search numbers in the next year, may have an increase in the number of applications, and thus the difficulty of admission. This would be useful information to determine changes in admission rate and exclusivity of schools in the future.

The audience for this project are high school juniors and seniors, and admission counselors going through the college application process. These students can use the forecasted search numbers and possible ranking changes to determine which schools they should apply to and which may be more or less competitive in the next year. This project can be expanded to include any set of schools in which a person wants to apply. 

## Initial Data Analysis

The first step in the time series process is to check if each dataset is a random walk. If so, this means that the best possible forecast for this dataset is a naive forecast and there is no need to apply any modelling. The following code does so for the top 5 schools' data.
```{r, message=FALSE, warning =FALSE}
library(forecast)
library(readr)
library(ggplot2)
library(stringr)
library(zoo)
library(Metrics)

file_paths = Sys.glob(file.path("./data", "*"))
data_sets = list()
college_names = list()

for (i in (1:length(file_paths))){
  
  college.data <- read_delim(file_paths[i], delim = ",", skip = 1)
  
  college_names[i] = str_extract(string= file_paths[i], pattern = "[a-z]*_admissions")
  
  colnames(college.data) <- c("Week", "Searches")
  
  college.ts <- ts(college.data$Searches, start = c(2020, 5), end = c(2025, 5), freq = 52) #weekly with a yearly season
  
  g = autoplot(college.ts) +
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  data_sets[[i]] <- college.ts
}
```


```{r Check for random walk}
for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  diff.ts <- diff(college.ts)
  
  print(Acf(diff.ts, main = college_names[i]))
}
```
All ACF plots show significant correlation after differencing meaning these time series are not random walks.

To further prove this, we will also perform hypothesis testing.

```{r}
for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  college.ar.1 <- Arima(college.ts, order = c(1, 0, 0))
  
  print(college_names[i])
  print(summary(college.ar.1))
  
  ar1_intercept <- summary(college.ar.1)$coef["ar1"]
  ar1_se <- sqrt(diag(college.ar.1$var.coef))['ar1']
  
  print(paste("Is significant?" , -1.96 > (ar1_intercept-1)/ar1_se))
}

```

We can conclude that all our coefficients are significantly different from 0. 

Null Hypothesis: beta = 1 (i.e., random walk)

Alternative Hypothesis: beta not equal to 1 (i.e., not random walk)

To be specific, our t-stat = (coefficient - 1)/s.e = 
t-stat = +-/1.96 at 95% confidence interval

All t-stat are < -1.96 so are significant, there is significant evidence to reject null hypothesis. These time series are not random walks.

## Time Series Process

For each school, we will perform the following process.

  1. Determine the best time time series for the data. Validation data will be the last 52 weeks (one year).
  
    - Seasonal Naive
    - Simple Exponential Smoothing
    - TSLM : Time Series Linear Model
    - Holt-Winters
    - Auto ARIMA
    - NNAR
  
  2. Find the best model for each school (comparing MAPE)
  3. Using the entire dataset as training and the best model from the previous step, forecast statistics for the next year (52 weeks).

The forecasts for each school will help determine how each school may rise or fall in popularity. 

## Time Series Process

For each school, we will perform the following process.

  1. Determine the best time time series for the data. Validation data will be the last 52 weeks (one year).
  
    - Seasonal Naive
    - Simple Exponential Smoothing
    - TSLM : Time Series Linear Model
    - Holt-Winters
    - Auto ARIMA
    - Moving Average
    - Holts Winters
    - Auto Arima
    - NNAR
  
  2. Find the best model for each school (comparing MAPE)
  3. Using the entire dataset as training and the best model from the previous step, forecast statistics for the next year (52 weeks).

The forecasts for each school will help determine how each school may rise or fall in popularity. 

#### Train/Valid Split
```{r, message = FALSE}
nValid <- 52
plots <- list()

train_data <- list()
valid_data <- list()
n_valid <- list()

for (i in 1:length(file_paths)) {
  
  college.ts <- data_sets[[i]]
  
  nTrain <- length(college.ts) - nValid
  
  train.ts <- window(college.ts, start = c(2020, 5), end = c(2020, nTrain))
  valid.ts <- window(college.ts, start = c(2020, nTrain + 1), end = c(2020, nTrain + nValid))
  
  # create train/valid for each school
  school <- sub("_.*", "", college_names[i])
  
  train_data[[school]] <- train.ts
  valid_data[[school]] <- valid.ts
  n_valid[[school]] <- nValid
  
  p <- autoplot(train.ts) + 
    autolayer(valid.ts) +
    ggtitle(paste("Train Test Split: ", school))+
    ylab("Interest")
  
  plots[[i]] <- p
}

plots
```

#### Seasonal Naive - SC
```{r}
snaive_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  seasonal.naive <- snaive(train.ts, h = nValid, level = 0)
  seasonal.naive.forecast <- forecast(seasonal.naive, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  snaive_models[[school]] <- seasonal.naive.forecast
  
  p = autoplot(seasonal.naive.forecast, series = "Seasonal Naive") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### Simple Exponential Smoothing - SC
```{r}
ses_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  ses <- ets(train.ts, model = "ANN", alpha = 0.5)
  ses.forecast <- forecast(ses, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  ses_models[[school]] <- ses.forecast
  
  p = autoplot(ses.forecast, series = "Simple Exponential Smoothing") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### TSLM : Time Series Linear Model - SC
```{r}
tslm_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  tslm <- tslm(train.ts ~ trend + season)
  tslm.forecast <- forecast(tslm, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  tslm_models[[school]] <- tslm.forecast
  
  p = autoplot(tslm.forecast, series = "Time Series Linear Model") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### Moving Average - IM
```{r}
moving_avg_trailing_models = list()
moving_avg_centered_models = list()

for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  ma.trailing <- rollmean(college.ts, k = 12, align = "right") #zoo
  ma.centered <- ma(college.ts, order = 12) #forecast
  
   # save to list
  school <- sub("_.*", "", college_names[i])
  moving_avg_trailing_models[[school]] <- ma.trailing
  moving_avg_centered_models[[school]] <- ma.centered
  
  g = autoplot(college.ts) +
  autolayer(ma.trailing, series="Trailing MA")+
  autolayer(ma.centered, series="Centered MA")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}
```

#### Holts Winters - IM
```{r}
holt_winters_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]

  hwin.mod <- ets(train.ts, model = "MAA")

  # create predictions
  hwin.pred <- forecast(hwin.mod, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  holt_winters_models[[school]] <- hwin.pred

  g = autoplot(college.ts) +
  autolayer(hwin.pred$mean, series = "Holt-Winters")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}
```

#### Auto Arima - SD
```{r}
auto_arima_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  auto.arima <- auto.arima(train.ts)
  arima.auto.forecast <- forecast(auto.arima, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  auto_arima_models[[school]] <- arima.auto.forecast
  
  p = autoplot(arima.auto.forecast, series = "Auto Arima") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### NNAR - SD
```{r}
nnar_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  p <- 12 # Number of previous time steps used for forecast
  P <- 1 # Number of previous seasonal values to use 
  size <- 7 # Number of hidden nodes 
  
  school.nnetar <- nnetar(train.ts, repeats = 20, p = p, P = P, size = size)
  nnetar.forecast <- forecast(school.nnetar, h = n_valid[[i]])
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  nnar_models[[school]] <- nnetar.forecast
  
  p = autoplot(nnetar.forecast, series = "NNAR") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### Compare Models for Each School
Using MAPE
```{r}
mape_values = c()

for (i in (1:length(file_paths))){
  
  snaive <- snaive_models[[i]]
  ses <- ses_models[[i]]
  tslm <- tslm_models[[i]]
  hwin <- hwin_models[[i]]
  arima <- auto_arima_models[[i]]
  ma_trailing <- moving_avg_trailing_models[[i]]
  ma_centered <- moving_avg_centered_models[[i]]
  nnar <- nnar_models[[i]]
  
  snaive_mape <- accuracy(snaive)["Test set", "MAPE"]
  ses_mape <- accuracy(ses)["Test set", "MAPE"]
  tslm_mape <- accuracy(tslm)["Test set", "MAPE"]
  hwin_mape <- accuracy(hwin)["Test set", "MAPE"]
  arima_mape <- accuracy(arima)["Test set", "MAPE"]
  ma_trailing_mape <- accuracy(ma_trailing)["Test set", "MAPE"]
  ma_centered_mape <- accuracy(ma_centered)["Test set", "MAPE"]
  nnar_mape <- accuracy(nnar)["Test set", "MAPE"]
  
  
  mape_values <- c(mape_values, 
                   snaive_mape, 
                   ses_mape, 
                   tslm_mape, 
                   hwin_mape, 
                   arima_mape, 
                   ma_trailing_mape, 
                   ma_centered_mape, 
                   nnar_mape)
}

model_names <- c("Snaive", "SES", "TSLM", "HWIN", "ARIMA", "MA Trailing", "MA Centered", "NNAR")

best_model_index <- which.min(mape_values)

best_model <- model_names[best_model_index]
best_mape <- mape_values[best_model_index]

cat("Best Model: ", best_model, "\n")
cat("MAPE: ", best_mape, "\n")
```

#### Forecast for entire period

