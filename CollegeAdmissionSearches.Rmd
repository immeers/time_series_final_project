---
title: "College Admission Searches"
author: "Imogen Meers, Sarah Deussing, & Sarah Cernugel"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
subtitle: "Will the Top-Ranked Schools Change?"
---

## Introduction/Outline
This analysis will forecast the number of Google Searches for admissions data of the top five nationally ranked U.S. universities, in addition to another college of interest. The current U.S. News rankings are as follows:

  1. Princeton University
  2. Massachusetts Institute of Technology
  3. Harvard University
  4. Stanford University
  5. Yale University
  18. University of Notre Dame

Data was obtained from Google Search statistics for each school; the search terms are "[school name] admission." Each dataset contains two columns: the date and interest for the term. These are weekly time series with five years of data.

The interest column is not a total number of searches. Instead, the values represent search interest relative to the maximum in the five year period. Values for interest are scaled to range from zero to one hundred. 

Time-series analysis will be performed to identify trends and seasonality in each dataset. Following, a forecast will predict future weekly search amounts. Doing so will determine if specific schools are projected to rise or fall in popularity.

## Initial Data Analysis & Visualization
```{r, message=FALSE, warning =FALSE}
library(forecast)
library(readr)
library(ggplot2)
library(stringr)
library(zoo)
library(dplyr)
library(gridExtra)
library(grid)

file_paths = Sys.glob(file.path("./data", "*"))
data_sets = list()
college_names = list()

for (i in (1:length(file_paths))){
  
  college.data <- read_delim(file_paths[i], delim = ",", skip = 1)
  
  college_names[i] = str_extract(string= file_paths[i], pattern = "[a-z]*_admissions")
  
  colnames(college.data) <- c("Week", "Searches")
  
  college.ts <- ts(college.data$Searches, start = c(2020, 7), end = c(2025, 7), freq = 52) #weekly with a yearly season
  
  g = autoplot(college.ts) +
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  data_sets[[i]] <- college.ts
}
```

#### Evaluating Predictability
The first step in the time series process is to check if each dataset is a random walk. If so, this means that the best possible forecast for this dataset is a naive forecast and there is no need to apply any modelling. The following code does so for all schools' data.
```{r Check for random walk}
acf_plots <- list()

for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  diff.ts <- diff(college.ts)
  acf_plots[[i]] <- Acf(diff.ts, main = college_names[i])
  
  print(acf_plots[[i]])
}
```


All ACF plots show significant correlation after differencing meaning these time series are not random walks.

To further prove this, we will also perform hypothesis testing.
```{r}
ar <- numeric(6)  
se <- numeric(6)
t_stat <- numeric(6)

for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  college.ar.1 <- Arima(college.ts, order = c(1, 0, 0))
  
  print(college_names[i])
  print(summary(college.ar.1))
  
  ar1_intercept <- summary(college.ar.1)$coef["ar1"]
  ar1_se <- sqrt(diag(college.ar.1$var.coef))['ar1']
  
  ar[i] <- ar1_intercept
  se[i] <- ar1_se
  t_stat[i] <- (ar1_intercept-1)/ar1_se
  
  print(paste("Is significant?" , -1.96 > (ar1_intercept-1)/ar1_se))
}
```

```{r}
random_walk_test <- data.frame("School" = unlist(college_names), "AR1_Coeff" = ar, "S.E" = se, "T_Stat" = t_stat, "Significant" = -1.96 > t_stat)
random_walk_test
```

We can conclude that all our coefficients are significantly different from 0. 

Null Hypothesis: beta = 1 (i.e., random walk)
Alternative Hypothesis: beta not equal to 1 (i.e., not random walk)

To be specific, our t-stat = (coefficient - 1)/s.e = 
t-stat = +-/1.96 at 95% confidence interval

All t-stat are < -1.96 so are significant, there is significant evidence to reject null hypothesis. These time series are not random walks. Therefore, we can continue with the time series process.

## Time Series Process & Forecasting Models
For each school, we will perform the following process.

  1. Determine the best time time series for the data. Nine time series will be created for each school
  
    1. Seasonal Naive
    2. Simple Exponential Smoothing
    3. TSLM : Time Series Linear Model
    4. Moving Average
    5. Holt-Winters
    6. Exponential Smoothing (ETS)
    7. Auto Arima
    8. Neural Network (NNAR)
    9. Sine/Cosine Encoding
  
  2. Find the best model for each school (comparing MAPE)
  3. Using the entire dataset as training and the best model from the previous step, forecast statistics for the next year (52 weeks).

The forecasts for each school will help determine how each school may rise or fall in popularity. 

#### Train/Valid Split
Validation data will be the last 52 weeks (one year).

Pre-processing steps include changes values of 0 to 1. Doing so will allow for the calculation of MAPE values. This change will keep these values still relatively low within the range of the time series values. 
```{r, message = FALSE}
library(gridExtra)
nValid <- 52

train_data <- list()
valid_data <- list()
n_valid <- list()
p_list <- list()

for (i in 1:length(file_paths)) {
  
  college.ts <- data_sets[[i]]
  
  # Zeros will affect training ability and MAPE calculations, so make these instances = 1.
  college.ts[college.ts == 0] <- 1
  
  nTrain <- length(college.ts) - nValid
  
  train.ts <- window(college.ts, start = c(2020, 7), end = c(2020, nTrain))
  valid.ts <- window(college.ts, start = c(2020, nTrain + 1), end = c(2020, nTrain + nValid))
  
  # create train/valid for each school
  school <- sub("_.*", "", college_names[i])
  
  train_data[[school]] <- train.ts
  valid_data[[school]] <- valid.ts
  n_valid[[school]] <- nValid
  
  p <- autoplot(train.ts) + 
    autolayer(valid.ts) +
    ggtitle(paste("Train Test Split: ", school))+
    ylab("Interest")
  
  p_list[[i]] = p
}

grid.arrange(p_list[[1]], p_list[[2]], p_list[[3]], p_list[[4]], p_list[[5]], p_list[[6]], ncol = 2)

```

All models listed above will be created for each schools' data.

#### (1) Seasonal Naive
```{r}
snaive_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  seasonal.naive <- snaive(train.ts, h = nValid, level = 0)
  seasonal.naive.forecast <- forecast(seasonal.naive, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  snaive_models[[school]] <- seasonal.naive.forecast
  
  p = autoplot(seasonal.naive.forecast, series = "Seasonal Naive") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### (2) Simple Exponential Smoothing
```{r}
ses_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # need to remove trend (lag1) + seasonality (lag52)
  diff.twice.train.ts <- diff(diff(train.ts, lag = 52), lag = 1)
  
  # create predictions
  ses <- ets(diff.twice.train.ts, model = "ANN", alpha = 0.5)
  ses.forecast <- forecast(ses, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  ses_models[[school]] <- ses.forecast
  
  p = autoplot(ses.forecast, series = "Simple Exponential Smoothing") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

```{r}
ses_models = list()

schools_to_save <- c("harvard", "mit", "nd", "princeton", "stanford", "yale")
alpha_values <- c(0.9, 0.9, 0.1, 0.7, 0.1, 0.1)

for (i in 1:length(file_paths)) {
  
  school <- sub("_.*", "", college_names[i]) 
  
  train.ts = train_data[[i]]
  
  # need to remove trend (lag1) + seasonality (lag52)
  diff.twice.train.ts <- diff(diff(train.ts, lag = 52), lag = 1)
  
  alpha_value <- alpha_values[which(schools_to_save == school)]
  
  # create predictions
  ses <- ets(diff.twice.train.ts, model = "ANN", alpha = alpha_value)
  ses.forecast <- forecast(ses, h = n_valid[[i]], level = 0)
  
  # save to list
  ses_models[[school]] <- ses.forecast

  p = autoplot(ses.forecast, series = paste("Simple Exponential Smoothing (alpha =", alpha_value, ")")) +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + 
    labs(title = paste(college_names[i], " - Alpha", alpha_value))
  
  print(p)
}

```

#### (3) TSLM : Time Series Linear Model
```{r}
tslm_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  tslm <- tslm(train.ts ~ trend + season) # linear trend + season
  tslm.forecast <- forecast(tslm, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  tslm_models[[school]] <- tslm.forecast
  
  p = autoplot(tslm.forecast, series = "Time Series Linear Model") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### (4) Moving Average
```{r}
moving_avg_trailing_models = list()
moving_avg_centered_models = list()

for (i in (1:length(file_paths))){
  
  college.ts = data_sets[[i]]

  ma.trailing <- rollmean(college.ts, k = 12, align = "right") #zoo
  ma.centered <- ma(college.ts, order = 12) #forecast
  
  last.ma.trailing <- tail(ma.trailing, 1) 
  last.ma.centered <- tail(ma.centered, 1) 
  
  nTrain <- length(college.ts) - n_valid[[i]]
  ma.trailing.pred <- ts(rep(last.ma.trailing, n_valid[[i]]), start = c(2020, nTrain + 1), 
                       end = c(2020, nTrain + n_valid[[i]]), freq = 52)
  ma.centered.pred <- ts(rep(last.ma.centered, n_valid[[i]]), start = c(2020, nTrain + 1), 
                       end = c(2020, nTrain + n_valid[[i]]), freq = 52)
  
   # save to list
  school <- sub("_.*", "", college_names[i])
  moving_avg_trailing_models[[school]] <- ma.trailing.pred
  moving_avg_centered_models[[school]] <- ma.centered.pred
  
  g = autoplot(college.ts) +
  autolayer(ma.trailing, series="Trailing MA")+
  autolayer(ma.centered, series="Centered MA")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}
```

#### (5) Holt-Winters
Could not evaluate because time series frequency was > 24. 
```{r, eval = FALSE}
holt_winters_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]

  hwin.mod <- ets(train.ts, model = "MAA")

  # create predictions
  hwin.pred <- forecast(hwin.mod, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  holt_winters_models[[school]] <- hwin.pred

  g = autoplot(college.ts) +
  autolayer(hwin.pred$mean, series = "Holt-Winters")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}
```

#### (6) ETS/STLF
```{r}
ets_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]

  stlf.model <- stlf(train.ts)

  # create predictions
  ets.pred <- forecast(stlf.model, h = n_valid[[i]])
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  ets_models[[school]] <- ets.pred

  g = autoplot(college.ts) +
  autolayer(ets.pred$mean, series = "ETS")+
  geom_line(color = "black") + labs(title = college_names[i])
  
  print(g)
  
}
```

#### (7) Auto Arima
```{r}
auto_arima_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  auto.arima <- auto.arima(train.ts)
  arima.auto.forecast <- forecast(auto.arima, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  auto_arima_models[[school]] <- arima.auto.forecast
  
  p = autoplot(arima.auto.forecast, series = "Auto Arima") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### (8) Neural Network Model: NNAR
```{r}
nnar_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  p <- 12 # Number of previous time steps used for forecast
  P <- 1 # Number of previous seasonal values to use 
  size <- 7 # Number of hidden nodes 
  
  school.nnetar <- nnetar(train.ts, repeats = 20, p = p, P = P, size = size)
  nnetar.forecast <- forecast(school.nnetar, h = n_valid[[i]])
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  nnar_models[[school]] <- nnetar.forecast
  
  p = autoplot(nnetar.forecast, series = "NNAR") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

#### (9) Sine/Cosine Encoding
```{r}
wave_models = list()

for (i in (1:length(file_paths))){
  
  train.ts = train_data[[i]]
  
  # create predictions
  wave <- tslm(train.ts ~ season + I(sin(2*pi*trend/52)) + I(cos(2*pi*trend/52)))
  wave.forecast <- forecast(wave, h = n_valid[[i]], level = 0)
  
  # save to list
  school <- sub("_.*", "", college_names[i])
  wave_models[[school]] <- wave.forecast
  
  p = autoplot(wave.forecast, series = "Model with Sine/Cosine Seasonality") +
    autolayer(valid_data[[i]], series = "Observed") +
    geom_line(color = "black") + labs(title = college_names[i])

  print(p)
}
```

All models have been created and stored for each school. Models will now be compared using MAPE values.

#### Compare Models for Each School
Mean Absolute Percentage Error (MAPE) calculates the percentage error for the models on the validation data. For each school, the model with the lowest MAPE is the model that fit the data best.
```{r}
model_names <- c("SNAIVE", "SES", "TSLM", "ARIMA", "MA TRAILING", "NNAR", "ETS", "WAVE")

for (i in (1:length(file_paths))){
  
  snaive <- snaive_models[[i]]
  ses <- ses_models[[i]]
  tslm <- tslm_models[[i]]
  arima <- auto_arima_models[[i]]
  ma_trailing <- moving_avg_trailing_models[[i]]
  nnar <- nnar_models[[i]]
  ets <- ets_models[[i]]
  wave <- wave_models[[i]]
  
  snaive_mape <- accuracy(snaive, valid_data[[i]])["Test set", "MAPE"]
  ses_mape <- accuracy(ses, valid_data[[i]])["Test set", "MAPE"]
  tslm_mape <- accuracy(tslm, valid_data[[i]])["Test set", "MAPE"]
  arima_mape <- accuracy(arima, valid_data[[i]])["Test set", "MAPE"]
  ma_trailing_mape <- accuracy(ma_trailing, valid_data[[i]])["MAPE"]
  nnar_mape <- accuracy(nnar, valid_data[[i]])["Test set", "MAPE"]
  ets_mape <- accuracy(ets, valid_data[[i]])["Test set", "MAPE"]
  wave_mape <- accuracy(wave, valid_data[[i]])["Test set", "MAPE"]
  
  mape_values <- c(snaive_mape, ses_mape, tslm_mape, arima_mape, ma_trailing_mape, nnar_mape, ets_mape, wave_mape)
  
  school <- sub("_.*", "", college_names[[i]])
  col_name <- paste(school, "Model", sep = " ")
  mape_df <- data.frame(Model = model_names, MAPE = mape_values)
  names(mape_df)[1] <- col_name
  
  print(mape_df)
  
  min_index <- which.min(mape_values)
  best_model <- model_names[min_index]
  if (i == 1){
    plot = data.frame(school = school, Model = best_model, MAPE = mape_values[min_index])
  } else{
    plot <- rbind.data.frame(plot, c(school, best_model, mape_values[min_index]))
  }
  
  
  cat(paste(school, "best model:", best_model, "\n"))
}
```

```{r}
plot$MAPE =as.numeric(plot$MAPE)
# Assuming your data frame is named 'plot'
ggplot(data = plot, aes(x = school, y = MAPE)) + 
  geom_bar(stat = "identity")
```

## Forecasting: Future Values
```{r}
best_plots = list()
change_plots = list()
```

The datasets are stored as:

  1: harvard, 2: mit, 3; nd, 4: princeton, 5: stanford, 6: yale

A forecast for the next year will be created for each school using their respective best model from above. Then, the forecasted values will be compared to the values from the current year (forecast - current). A positive value represents a forecasted increase for 2025/26.
  
Harvard: Sine/Cosine Encoding (Wave)
```{r, message = False}
harvard.full <- tslm(data_sets[[1]] ~ season + I(sin(2*pi*trend/52)) + I(cos(2*pi*trend/52)))
harvard.forecast <- forecast(harvard.full, h = 52)
best_plots[[1]] = autoplot(harvard.forecast) + ggtitle("Harvard Forecast") + ylab("Search Interest")

print(best_plots[[1]])

# Look at August 1 - January 1 (application period)
college.data <- read_delim(file_paths[1], delim = ",", skip = 1)
app.period <- college.data %>% 
  filter(Week >= as.Date('2024-08-01') & Week <= as.Date('2025-01-01'))

last_date <- tail(college.data$Week, 1)
forecast.dates <- seq(from = last_date + 1, by = "week", length.out = length(harvard.forecast$mean))
forecast.data <- data.frame(Date = forecast.dates, Forecast = harvard.forecast$mean)
forecast.app.period <- forecast.data %>% 
  filter(Date >= as.Date('2025-08-01') & Date <= as.Date('2026-01-01'))

change <- forecast.app.period$Forecast - app.period$`harvard admissions: (United States)`
differences <- data.frame(Date = app.period$Week, Change = change)
differences$Date <- as.Date(differences$Date)

change_plots[[1]] = ggplot(differences, aes(x = Date, y = Change)) + 
  geom_line() +
  ggtitle("Harvard's Next Application Cycle - 2025/26") +
  geom_vline(xintercept = as.Date(c('2024-11-01', '2025-01-01')), 
             linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", color = "blue") +
  labs(x = "Date", y = "Change") +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")


print(change_plots[[1]])
```

MIT: ARIMA
```{r, message = False}
mit.full <- auto.arima(data_sets[[2]])
mit.forecast <- forecast(mit.full, h = 52)
best_plots[[2]] = autoplot(mit.forecast) + ggtitle("MIT Forecast") + ylab("Search Interest")

print(best_plots[[2]])

# Look at August 1 - January 1 (application period)
college.data <- read_delim(file_paths[2], delim = ",", skip = 1)
app.period <- college.data %>% 
  filter(Week >= as.Date('2024-08-01') & Week <= as.Date('2025-01-01'))

last_date <- tail(college.data$Week, 1)
forecast.dates <- seq(from = last_date + 1, by = "week", length.out = length(mit.forecast$mean))
forecast.data <- data.frame(Date = forecast.dates, Forecast = mit.forecast$mean)
forecast.app.period <- forecast.data %>% 
  filter(Date >= as.Date('2025-08-01') & Date <= as.Date('2026-01-01'))

change <- forecast.app.period$Forecast - app.period$`mit admissions: (United States)`
differences <- data.frame(Date = app.period$Week, Change = change)
differences$Date <- as.Date(differences$Date)

change_plots[[2]] = ggplot(differences, aes(x = Date, y = Change)) + 
  geom_line() +
  ggtitle("MIT's Next Application Cycle - 2025/26") +
  geom_vline(xintercept = as.Date(c('2024-11-01', '2025-01-01')), 
             linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", color = "blue") +

  labs(x = "Date", y = "Change") +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")


print(change_plots[[2]])
```

Notre Dame: Simple Exponential Smoothing
```{r, message = False}
#nd.full <- tslm(data_sets[[3]] ~ season + I(sin(2*pi*trend/52)) + I(cos(2*pi*trend/52)))
#nd.forecast <- forecast(nd.full, h = 52)

# need to remove trend (lag1) + seasonality (lag52)
nd.full <- data_sets[[3]]
nd.full.diff <- diff(diff(nd.full, lag = 52), lag = 1)
nd.ses <- ets(nd.full.diff, model = "ANN", alpha = 0.5)
nd.forecast <- forecast(nd.ses, h = 52)

best_plots[[3]] = autoplot(nd.forecast) + autolayer(nd.full) + ggtitle("Notre Dame Forecast") + ylab("Search Interest")


print(best_plots[[3]])
# Look at August 1 - January 1 (application period)
college.data <- read_delim(file_paths[3], delim = ",", skip = 1)
app.period <- college.data %>% 
  filter(Week >= as.Date('2024-08-01') & Week <= as.Date('2025-01-01'))

last_date <- tail(college.data$Week, 1)
forecast.dates <- seq(from = last_date + 1, by = "week", length.out = length(nd.forecast$mean))
forecast.data <- data.frame(Date = forecast.dates, Forecast = nd.forecast$mean)
forecast.app.period <- forecast.data %>% 
  filter(Date >= as.Date('2025-08-01') & Date <= as.Date('2026-01-01'))

change <- forecast.app.period$Forecast - app.period$`notre dame admissions: (United States)`
differences <- data.frame(Date = app.period$Week, Change = change)
differences$Date <- as.Date(differences$Date)

change_plots[[3]] = ggplot(differences, aes(x = Date, y = Change)) + 
  geom_line() +
  ggtitle("Notre Dame's Next Application Cycle - 2025/26") +
  geom_vline(xintercept = as.Date(c('2024-11-01', '2025-01-01')), 
             linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", color = "blue") +

  labs(x = "Date", y = "Change") +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")

print(change_plots[[3]])
```

Princeton: Sine/Cosine Encoding (Wave)
```{r, message = False}
princeton.full <- tslm(data_sets[[4]] ~ season + I(sin(2*pi*trend/52)) + I(cos(2*pi*trend/52)))
princeton.forecast <- forecast(princeton.full, h = 52)
best_plots[[4]] = autoplot(princeton.forecast) + ggtitle("Princeton Forecast") + ylab("Search Interest")

print(best_plots[[4]])

# Look at August 1 - January 1 (application period)
college.data <- read_delim(file_paths[4], delim = ",", skip = 1)
app.period <- college.data %>% 
  filter(Week >= as.Date('2024-08-01') & Week <= as.Date('2025-01-01'))

last_date <- tail(college.data$Week, 1)
forecast.dates <- seq(from = last_date + 1, by = "week", length.out = length(princeton.forecast$mean))
forecast.data <- data.frame(Date = forecast.dates, Forecast = princeton.forecast$mean)
forecast.app.period <- forecast.data %>% 
  filter(Date >= as.Date('2025-08-01') & Date <= as.Date('2026-01-01'))

change <- forecast.app.period$Forecast - app.period$`princeton admissions: (United States)`
differences <- data.frame(Date = app.period$Week, Change = change)
differences$Date <- as.Date(differences$Date)

change_plots[[4]] = ggplot(differences, aes(x = Date, y = Change)) + 
  geom_line() +
  ggtitle("Princeton's Next Application Cycle - 2025/26") +
  geom_vline(xintercept = as.Date(c('2024-11-01', '2025-01-01')), 
             linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", color = "blue") +
  labs(x = "Date", y = "Change") +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")

print(change_plots[[4]])
```

Stanford: ARIMA
```{r, message = False}
stanford.full <- auto.arima(data_sets[[5]])
stanford.forecast <- forecast(stanford.full, h = 52)
best_plots[[5]] = autoplot(stanford.forecast) + ggtitle("Stanford Forecast") + ylab("Search Interest")


print(best_plots[[5]])
# Look at August 1 - January 1 (application period)
college.data <- read_delim(file_paths[5], delim = ",", skip = 1)
app.period <- college.data %>% 
  filter(Week >= as.Date('2024-08-01') & Week <= as.Date('2025-01-01'))

last_date <- tail(college.data$Week, 1)
forecast.dates <- seq(from = last_date + 1, by = "week", length.out = length(stanford.forecast$mean))
forecast.data <- data.frame(Date = forecast.dates, Forecast = stanford.forecast$mean)
forecast.app.period <- forecast.data %>% 
  filter(Date >= as.Date('2025-08-01') & Date <= as.Date('2026-01-01'))

change <- forecast.app.period$Forecast - app.period$`stanford admissions: (United States)`
differences <- data.frame(Date = app.period$Week, Change = change)
differences$Date <- as.Date(differences$Date)

change_plots[[5]] = ggplot(differences, aes(x = Date, y = Change)) + 
  geom_line() +
  ggtitle("Stanford's Next Application Cycle - 2025/26") +
  geom_vline(xintercept = as.Date(c('2024-11-01', '2025-01-01')), 
             linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", color = "blue") +

  labs(x = "Date", y = "Change") +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")

print(change_plots[[5]])
```

Yale: ETS
```{r, message = False}
yale.full <- stlf(data_sets[[6]])
yale.forecast <- forecast(yale.full, h = 52)
best_plots[[6]] = autoplot(yale.forecast) + ggtitle("Yale Forecast") + ylab("Search Interest")


print(best_plots[[6]])
# Look at August 1 - January 1 (application period)
college.data <- read_delim(file_paths[6], delim = ",", skip = 1)
app.period <- college.data %>% 
  filter(Week >= as.Date('2024-08-01') & Week <= as.Date('2025-01-01'))

last_date <- tail(college.data$Week, 1)
forecast.dates <- seq(from = last_date + 1, by = "week", length.out = length(yale.forecast$mean))
forecast.data <- data.frame(Date = forecast.dates, Forecast = yale.forecast$mean)
forecast.app.period <- forecast.data %>% 
  filter(Date >= as.Date('2025-08-01') & Date <= as.Date('2026-01-01'))

change <- forecast.app.period$Forecast - app.period$`yale admissions: (United States)`
differences <- data.frame(Date = app.period$Week, Change = change)
differences$Date <- as.Date(differences$Date)

change_plots[[6]] = ggplot(differences, aes(x = Date, y = Change)) + 
  geom_line() +
  ggtitle("Yale's Next Application Cycle - 2025/26") +
  geom_vline(xintercept = as.Date(c('2024-11-01', '2025-01-01')), 
             linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", color = "blue") +

  labs(x = "Date", y = "Change") +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")

print(change_plots[[6]])
```

Forecast Plots
```{r, eval = FALSE}
grid.arrange(best_plots[[1]], best_plots[[2]], best_plots[[3]], best_plots[[4]], best_plots[[5]], best_plots[[6]], ncol = 2)
```

Forecast Change Plots
```{r, eval = FALSE}
grid.arrange(change_plots[[1]], change_plots[[2]], change_plots[[3]], change_plots[[4]], change_plots[[5]], change_plots[[6]], ncol = 2)
```
